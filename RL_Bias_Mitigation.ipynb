{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e0b276",
   "metadata": {},
   "source": [
    "# RL Bias Mitigation Project - Phase 1\n",
    "## Loan Approval System with Fairness Constraints\n",
    "\n",
    "**Group 3**: Aniketh (AM.EN.UA41E22009), Jatin (AM.EN.UA41E22024), Kaushik (AM.EN.UA41E22026)\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Loan approval systems often embed historical human biases. This project uses **Reinforcement Learning (DQN)** to learn fair loan approval decisions while reducing gender-based disparity.\n",
    "\n",
    "**Baseline Data Statistics:**\n",
    "- Men approval rate: **42.93%**\n",
    "- Women approval rate: **18.92%**\n",
    "- Goal: Reduce this disparity through fairness-aware RL\n",
    "\n",
    "---\n",
    "\n",
    "### MDP Formulation\n",
    "\n",
    "- **State Space (S)**: `(salary, years_exp, sex)`\n",
    "- **Action Space (A)**: `{0: Reject, 1: Approve}`\n",
    "- **Reward Function (R)**: `R_classification + R_fairness`\n",
    "  - Classification: +1 (correct approve), -1 (wrong approve), 0 (reject)\n",
    "  - Fairness penalty: `-Œª * |approval_rate(women) - approval_rate(men)|`\n",
    "- **Discount Factor (Œ≥)**: 0.99\n",
    "- **Episode Length**: 100 applicants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f25fcb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120487a",
   "metadata": {},
   "source": [
    "## 2. Load and Analyze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('biased_gender_loans.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze baseline bias\n",
    "men_data = data[data['sex'] == 'Man']\n",
    "women_data = data[data['sex'] == 'Woman']\n",
    "\n",
    "approval_rate_men = (men_data['bank_loan'] == 'Yes').sum() / len(men_data)\n",
    "approval_rate_women = (women_data['bank_loan'] == 'Yes').sum() / len(women_data)\n",
    "spd_baseline = approval_rate_women - approval_rate_men\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE DATA ANALYSIS (Biased Historical Data)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Applicants: {len(data)}\")\n",
    "print(f\"  Men: {len(men_data)}\")\n",
    "print(f\"  Women: {len(women_data)}\")\n",
    "print(f\"\\nApproval Rates:\")\n",
    "print(f\"  Men: {approval_rate_men:.2%}\")\n",
    "print(f\"  Women: {approval_rate_women:.2%}\")\n",
    "print(f\"\\nFairness Metrics:\")\n",
    "print(f\"  Statistical Parity Difference: {spd_baseline:.4f}\")\n",
    "print(f\"  Absolute Bias Gap: {abs(spd_baseline):.2%}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95651669",
   "metadata": {},
   "source": [
    "## 3. Visualize Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc756d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Dataset Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Salary distribution by gender\n",
    "axes[0, 0].hist(men_data['salary'], bins=30, alpha=0.6, label='Men', color='blue')\n",
    "axes[0, 0].hist(women_data['salary'], bins=30, alpha=0.6, label='Women', color='orange')\n",
    "axes[0, 0].set_xlabel('Salary')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Salary Distribution by Gender')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Experience distribution by gender\n",
    "axes[0, 1].hist(men_data['years_exp'], bins=20, alpha=0.6, label='Men', color='blue')\n",
    "axes[0, 1].hist(women_data['years_exp'], bins=20, alpha=0.6, label='Women', color='orange')\n",
    "axes[0, 1].set_xlabel('Years of Experience')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Experience Distribution by Gender')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Approval rate by gender\n",
    "approval_counts = data.groupby(['sex', 'bank_loan']).size().unstack(fill_value=0)\n",
    "approval_counts.plot(kind='bar', ax=axes[1, 0], color=['indianred', 'steelblue'])\n",
    "axes[1, 0].set_xlabel('Gender')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Loan Approval Counts by Gender')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=0)\n",
    "axes[1, 0].legend(title='Loan Approved')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Salary vs Experience colored by approval\n",
    "approved = data[data['bank_loan'] == 'Yes']\n",
    "rejected = data[data['bank_loan'] == 'No']\n",
    "axes[1, 1].scatter(rejected['years_exp'], rejected['salary'], alpha=0.4, s=20, c='red', label='Rejected')\n",
    "axes[1, 1].scatter(approved['years_exp'], approved['salary'], alpha=0.4, s=20, c='green', label='Approved')\n",
    "axes[1, 1].set_xlabel('Years of Experience')\n",
    "axes[1, 1].set_ylabel('Salary')\n",
    "axes[1, 1].set_title('Salary vs Experience (Colored by Approval)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b546c",
   "metadata": {},
   "source": [
    "## 4. Define Loan Approval Environment (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoanApprovalEnv:\n",
    "    \"\"\"Custom Environment for Loan Approval with Fairness Constraints\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, episode_length: int = 100, lambda_fairness: float = 0.5):\n",
    "        self.data_original = data.copy()\n",
    "        self.episode_length = episode_length\n",
    "        self.lambda_fairness = lambda_fairness\n",
    "        \n",
    "        # Normalize continuous features\n",
    "        self.salary_min = data['salary'].min()\n",
    "        self.salary_max = data['salary'].max()\n",
    "        self.exp_min = data['years_exp'].min()\n",
    "        self.exp_max = data['years_exp'].max()\n",
    "        \n",
    "        self.data = data.copy()\n",
    "        self.data['salary_norm'] = (data['salary'] - self.salary_min) / (self.salary_max - self.salary_min)\n",
    "        self.data['exp_norm'] = (data['years_exp'] - self.exp_min) / (self.exp_max - self.exp_min)\n",
    "        self.data['sex_encoded'] = (data['sex'] == 'Man').astype(float)\n",
    "        self.data['label'] = (data['bank_loan'] == 'Yes').astype(int)\n",
    "        \n",
    "        # Episode tracking\n",
    "        self.current_step = 0\n",
    "        self.current_state = None\n",
    "        self.current_label = None\n",
    "        self.current_sex = None\n",
    "        \n",
    "        # Fairness tracking\n",
    "        self.episode_approvals = {'Man': [], 'Woman': []}\n",
    "        self.episode_total = {'Man': 0, 'Woman': 0}\n",
    "        self.total_approvals = {'Man': 0, 'Woman': 0}\n",
    "        self.total_counts = {'Man': 0, 'Woman': 0}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment for new episode\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.episode_approvals = {'Man': [], 'Woman': []}\n",
    "        self.episode_total = {'Man': 0, 'Woman': 0}\n",
    "        return self._get_next_applicant()\n",
    "    \n",
    "    def _get_next_applicant(self):\n",
    "        \"\"\"Sample random applicant from dataset\"\"\"\n",
    "        idx = np.random.randint(0, len(self.data))\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        self.current_state = np.array([\n",
    "            row['salary_norm'],\n",
    "            row['exp_norm'],\n",
    "            row['sex_encoded']\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.current_label = row['label']\n",
    "        self.current_sex = row['sex']\n",
    "        self.episode_total[self.current_sex] += 1\n",
    "        self.total_counts[self.current_sex] += 1\n",
    "        \n",
    "        return self.current_state\n",
    "    \n",
    "    def _compute_classification_reward(self, action: int):\n",
    "        \"\"\"Compute base classification reward\"\"\"\n",
    "        if action == 1:  # Approve\n",
    "            return 1.0 if self.current_label == 1 else -1.0\n",
    "        else:  # Reject\n",
    "            return 0.0\n",
    "    \n",
    "    def _compute_fairness_penalty(self):\n",
    "        \"\"\"Compute fairness penalty based on approval rate disparity\"\"\"\n",
    "        approval_rate_man = (\n",
    "            np.mean(self.episode_approvals['Man']) \n",
    "            if len(self.episode_approvals['Man']) > 0 else 0.0\n",
    "        )\n",
    "        approval_rate_woman = (\n",
    "            np.mean(self.episode_approvals['Woman']) \n",
    "            if len(self.episode_approvals['Woman']) > 0 else 0.0\n",
    "        )\n",
    "        \n",
    "        gap = abs(approval_rate_woman - approval_rate_man)\n",
    "        penalty = -self.lambda_fairness * gap\n",
    "        return float(penalty)\n",
    "    \n",
    "    def step(self, action: int):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        # Record approval decision\n",
    "        self.episode_approvals[self.current_sex].append(float(action))\n",
    "        if action == 1:\n",
    "            self.total_approvals[self.current_sex] += 1\n",
    "        \n",
    "        # Compute rewards\n",
    "        classification_reward = self._compute_classification_reward(action)\n",
    "        fairness_penalty = self._compute_fairness_penalty()\n",
    "        total_reward = classification_reward + fairness_penalty\n",
    "        \n",
    "        # Increment step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.episode_length\n",
    "        \n",
    "        # Get next applicant\n",
    "        if not done:\n",
    "            next_state = self._get_next_applicant()\n",
    "        else:\n",
    "            next_state = self.current_state\n",
    "        \n",
    "        info = {\n",
    "            'classification_reward': classification_reward,\n",
    "            'fairness_penalty': fairness_penalty,\n",
    "            'current_sex': self.current_sex,\n",
    "            'action': action,\n",
    "            'label': self.current_label\n",
    "        }\n",
    "        \n",
    "        return next_state, total_reward, done, info\n",
    "    \n",
    "    def get_fairness_metrics(self):\n",
    "        \"\"\"Calculate fairness metrics\"\"\"\n",
    "        approval_rate_man = (\n",
    "            self.total_approvals['Man'] / self.total_counts['Man']\n",
    "            if self.total_counts['Man'] > 0 else 0.0\n",
    "        )\n",
    "        approval_rate_woman = (\n",
    "            self.total_approvals['Woman'] / self.total_counts['Woman']\n",
    "            if self.total_counts['Woman'] > 0 else 0.0\n",
    "        )\n",
    "        \n",
    "        spd = approval_rate_woman - approval_rate_man\n",
    "        \n",
    "        return {\n",
    "            'approval_rate_men': approval_rate_man,\n",
    "            'approval_rate_women': approval_rate_woman,\n",
    "            'statistical_parity_difference': spd,\n",
    "            'disparity_ratio': approval_rate_woman / approval_rate_man if approval_rate_man > 0 else 0.0,\n",
    "            'total_approvals_men': self.total_approvals['Man'],\n",
    "            'total_approvals_women': self.total_approvals['Woman'],\n",
    "            'total_men': self.total_counts['Man'],\n",
    "            'total_women': self.total_counts['Woman']\n",
    "        }\n",
    "\n",
    "print(\"‚úì LoanApprovalEnv class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a0bd7",
   "metadata": {},
   "source": [
    "## 5. Define DQN Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience tuple for replay buffer\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network for approximating Q-values\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=3, action_dim=2, hidden_dims=[64, 64]):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[1], action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(Experience(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array([e.state for e in experiences]))\n",
    "        actions = torch.LongTensor(np.array([e.action for e in experiences]))\n",
    "        rewards = torch.FloatTensor(np.array([e.reward for e in experiences]))\n",
    "        next_states = torch.FloatTensor(np.array([e.next_state for e in experiences]))\n",
    "        dones = torch.FloatTensor(np.array([e.done for e in experiences]))\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with Œµ-greedy exploration and target network\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim=3,\n",
    "        action_dim=2,\n",
    "        learning_rate=0.001,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=10,\n",
    "        device=None\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # Q-network and target network\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        \n",
    "        # Training step counter\n",
    "        self.training_steps = 0\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using Œµ-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample mini-batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Move to device\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update training steps\n",
    "        self.training_steps += 1\n",
    "        \n",
    "        # Update target network\n",
    "        if self.training_steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"‚úì DQN components defined (QNetwork, ReplayBuffer, DQNAgent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03104e0d",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b769c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "NUM_EPISODES = 1000\n",
    "EPISODE_LENGTH = 100\n",
    "LAMBDA_FAIRNESS = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "PRINT_FREQ = 50\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Episodes: {NUM_EPISODES}\")\n",
    "print(f\"  Episode Length: {EPISODE_LENGTH}\")\n",
    "print(f\"  Lambda (Fairness): {LAMBDA_FAIRNESS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Gamma (Discount): {GAMMA}\")\n",
    "print(f\"  Epsilon: {EPSILON_START} ‚Üí {EPSILON_END} (decay: {EPSILON_DECAY})\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Target Update Freq: {TARGET_UPDATE_FREQ}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f3528",
   "metadata": {},
   "source": [
    "## 7. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = LoanApprovalEnv(\n",
    "    data=data,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    lambda_fairness=LAMBDA_FAIRNESS\n",
    ")\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=3,\n",
    "    action_dim=2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    epsilon_start=EPSILON_START,\n",
    "    epsilon_end=EPSILON_END,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_update_freq=TARGET_UPDATE_FREQ,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úì Environment and Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f181e74",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dafa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics tracking\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "fairness_metrics_history = []\n",
    "epsilon_history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_losses_temp = []\n",
    "    step = 0\n",
    "    \n",
    "    for step in range(EPISODE_LENGTH):\n",
    "        # Select action\n",
    "        action = agent.select_action(state, training=True)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store experience\n",
    "        agent.store_experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train agent\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            episode_losses_temp.append(loss)\n",
    "        \n",
    "        # Update\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "    \n",
    "    # Record metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    avg_loss = np.mean(episode_losses_temp) if episode_losses_temp else 0\n",
    "    episode_losses.append(avg_loss)\n",
    "    fairness_metrics = env.get_fairness_metrics()\n",
    "    fairness_metrics_history.append(fairness_metrics)\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % PRINT_FREQ == 0:\n",
    "        recent_rewards = episode_rewards[-PRINT_FREQ:]\n",
    "        recent_losses = [l for l in episode_losses[-PRINT_FREQ:] if l > 0]\n",
    "        recent_spd = [m['statistical_parity_difference'] for m in fairness_metrics_history[-PRINT_FREQ:]]\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{NUM_EPISODES} | \"\n",
    "              f\"Avg Reward: {np.mean(recent_rewards):.2f} | \"\n",
    "              f\"Avg Loss: {np.mean(recent_losses):.4f} | \"\n",
    "              f\"SPD: {np.mean(recent_spd):.4f} | \"\n",
    "              f\"Œµ: {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca7668",
   "metadata": {},
   "source": [
    "## 9. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment statistics for evaluation\n",
    "env.total_approvals = {'Man': 0, 'Woman': 0}\n",
    "env.total_counts = {'Man': 0, 'Woman': 0}\n",
    "\n",
    "# Evaluate for 100 episodes\n",
    "eval_episodes = 100\n",
    "eval_rewards = []\n",
    "\n",
    "print(f\"Evaluating agent over {eval_episodes} episodes...\")\n",
    "\n",
    "for episode in range(eval_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(EPISODE_LENGTH):\n",
    "        action = agent.select_action(state, training=False)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "\n",
    "# Get evaluation metrics\n",
    "eval_metrics = env.get_fairness_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average Reward: {np.mean(eval_rewards):.2f} ¬± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"\\nApproval Rates:\")\n",
    "print(f\"  Men: {eval_metrics['approval_rate_men']:.2%}\")\n",
    "print(f\"  Women: {eval_metrics['approval_rate_women']:.2%}\")\n",
    "print(f\"\\nFairness Metrics:\")\n",
    "print(f\"  Statistical Parity Difference: {eval_metrics['statistical_parity_difference']:.4f}\")\n",
    "print(f\"  Disparity Ratio: {eval_metrics['disparity_ratio']:.3f}\")\n",
    "print(f\"\\nComparison with Baseline:\")\n",
    "print(f\"  Baseline SPD: {spd_baseline:.4f}\")\n",
    "print(f\"  Trained SPD: {eval_metrics['statistical_parity_difference']:.4f}\")\n",
    "improvement = abs(spd_baseline) - abs(eval_metrics['statistical_parity_difference'])\n",
    "print(f\"  SPD Improvement: {improvement:.4f} ({(improvement/abs(spd_baseline)*100):.1f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f002dc3",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('DQN Training Metrics for Bias Mitigation', fontsize=16, fontweight='bold')\n",
    "\n",
    "episodes = range(1, len(episode_rewards) + 1)\n",
    "window = 50\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "axes[0, 0].plot(episodes, episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "if len(episode_rewards) >= window:\n",
    "    moving_avg = pd.Series(episode_rewards).rolling(window=window).mean()\n",
    "    axes[0, 0].plot(episodes, moving_avg, color='red', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "valid_losses = [l for l in episode_losses if l > 0]\n",
    "valid_episodes = [i+1 for i, l in enumerate(episode_losses) if l > 0]\n",
    "axes[0, 1].plot(valid_episodes, valid_losses, alpha=0.6, label='Loss')\n",
    "if len(valid_losses) >= window:\n",
    "    moving_avg_loss = pd.Series(valid_losses).rolling(window=window).mean()\n",
    "    axes[0, 1].plot(valid_episodes, moving_avg_loss, color='red', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Average Loss')\n",
    "axes[0, 1].set_title('Training Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Statistical Parity Difference\n",
    "spd_values = [m['statistical_parity_difference'] for m in fairness_metrics_history]\n",
    "axes[1, 0].plot(episodes, spd_values, alpha=0.6, label='SPD')\n",
    "axes[1, 0].axhline(y=0, color='green', linestyle='--', linewidth=2, label='Perfect Parity')\n",
    "if len(spd_values) >= window:\n",
    "    moving_avg_spd = pd.Series(spd_values).rolling(window=window).mean()\n",
    "    axes[1, 0].plot(episodes, moving_avg_spd, color='red', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('SPD')\n",
    "axes[1, 0].set_title('Statistical Parity Difference (Women - Men)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Approval Rates by Gender\n",
    "approval_men = [m['approval_rate_men'] for m in fairness_metrics_history]\n",
    "approval_women = [m['approval_rate_women'] for m in fairness_metrics_history]\n",
    "axes[1, 1].plot(episodes, approval_men, alpha=0.6, label='Men', color='blue')\n",
    "axes[1, 1].plot(episodes, approval_women, alpha=0.6, label='Women', color='orange')\n",
    "if len(approval_men) >= window:\n",
    "    moving_avg_men = pd.Series(approval_men).rolling(window=window).mean()\n",
    "    moving_avg_women = pd.Series(approval_women).rolling(window=window).mean()\n",
    "    axes[1, 1].plot(episodes, moving_avg_men, color='darkblue', linewidth=2, label=f'Men ({window}-ep avg)')\n",
    "    axes[1, 1].plot(episodes, moving_avg_women, color='darkorange', linewidth=2, label=f'Women ({window}-ep avg)')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Approval Rate')\n",
    "axes[1, 1].set_title('Approval Rates by Gender')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df6fa8",
   "metadata": {},
   "source": [
    "## 11. Fairness Comparison: Baseline vs RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479eb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Fairness Improvement: Baseline vs RL Agent', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Approval Rates Comparison\n",
    "categories = ['Men', 'Women']\n",
    "baseline_rates = [approval_rate_men, approval_rate_women]\n",
    "trained_rates = [eval_metrics['approval_rate_men'], eval_metrics['approval_rate_women']]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, baseline_rates, width, label='Baseline (Biased)', color='indianred', alpha=0.8)\n",
    "axes[0].bar(x + width/2, trained_rates, width, label='RL Agent', color='steelblue', alpha=0.8)\n",
    "axes[0].set_ylabel('Approval Rate')\n",
    "axes[0].set_title('Approval Rates by Gender')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(categories)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (b, t) in enumerate(zip(baseline_rates, trained_rates)):\n",
    "    axes[0].text(i - width/2, b + 0.01, f'{b:.2%}', ha='center', va='bottom', fontsize=10)\n",
    "    axes[0].text(i + width/2, t + 0.01, f'{t:.2%}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 2: Statistical Parity Difference\n",
    "spd_baseline_abs = abs(spd_baseline)\n",
    "spd_trained_abs = abs(eval_metrics['statistical_parity_difference'])\n",
    "\n",
    "axes[1].bar(['Baseline', 'RL Agent'], [spd_baseline_abs, spd_trained_abs], \n",
    "            color=['indianred', 'steelblue'], alpha=0.8)\n",
    "axes[1].set_ylabel('Absolute SPD')\n",
    "axes[1].set_title('Statistical Parity Difference (Absolute)')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "axes[1].text(0, spd_baseline_abs + 0.005, f'{spd_baseline_abs:.4f}', ha='center', va='bottom', fontsize=11)\n",
    "axes[1].text(1, spd_trained_abs + 0.005, f'{spd_trained_abs:.4f}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# Add improvement percentage\n",
    "improvement_pct = ((spd_baseline_abs - spd_trained_abs) / spd_baseline_abs) * 100\n",
    "axes[1].text(0.5, max(spd_baseline_abs, spd_trained_abs) * 0.5, \n",
    "            f'Improvement:\\n{improvement_pct:.1f}%', \n",
    "            ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c597df1",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c56945",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìä BASELINE (Historical Biased Data)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Approval Rate (Men):    {approval_rate_men:.2%}\")\n",
    "print(f\"Approval Rate (Women):  {approval_rate_women:.2%}\")\n",
    "print(f\"SPD:                    {spd_baseline:.4f}\")\n",
    "print(f\"Absolute Bias Gap:      {abs(spd_baseline):.2%}\")\n",
    "\n",
    "print(\"\\nü§ñ TRAINED RL AGENT (DQN)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Approval Rate (Men):    {eval_metrics['approval_rate_men']:.2%}\")\n",
    "print(f\"Approval Rate (Women):  {eval_metrics['approval_rate_women']:.2%}\")\n",
    "print(f\"SPD:                    {eval_metrics['statistical_parity_difference']:.4f}\")\n",
    "print(f\"Absolute Bias Gap:      {abs(eval_metrics['statistical_parity_difference']):.2%}\")\n",
    "\n",
    "print(\"\\nüìà IMPROVEMENT\")\n",
    "print(\"-\" * 80)\n",
    "spd_reduction = abs(spd_baseline) - abs(eval_metrics['statistical_parity_difference'])\n",
    "spd_reduction_pct = (spd_reduction / abs(spd_baseline)) * 100\n",
    "print(f\"SPD Reduction:          {spd_reduction:.4f} ({spd_reduction_pct:.1f}% improvement)\")\n",
    "print(f\"Disparity Ratio Change: {approval_rate_women/approval_rate_men:.3f} ‚Üí {eval_metrics['disparity_ratio']:.3f}\")\n",
    "\n",
    "if spd_reduction > 0:\n",
    "    print(f\"\\n‚úÖ SUCCESS: The RL agent successfully reduced gender bias by {spd_reduction_pct:.1f}%!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Further tuning may be needed.\")\n",
    "\n",
    "print(\"\\nüéØ KEY TAKEAWAYS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. DQN successfully learned to balance accuracy and fairness\")\n",
    "print(\"2. Fairness penalty in reward function guided equitable decisions\")\n",
    "print(\"3. Statistical Parity Difference significantly reduced\")\n",
    "print(\"4. Both genders now have more balanced approval rates\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 15 + \"RL Bias Mitigation - Phase 1 Complete\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
